{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22f8368b",
   "metadata": {},
   "source": [
    "## Author : Ibrahim Sobh\n",
    "### Kaggle House Prices - Advanced Regression Technique.\n",
    "- Predict sales prices and practice feature engineering, RFs, and gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc35eb0",
   "metadata": {},
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078f9904",
   "metadata": {},
   "source": [
    "# I - First Section :  Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7176df6d",
   "metadata": {},
   "source": [
    "## 1 - Importing Libraries & Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7a726241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sklearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_log_error,r2_score\n",
    "from sklearn.preprocessing import OrdinalEncoder,StandardScaler\n",
    "\n",
    "# jobLib \n",
    "from joblib import dump,load\n",
    "\n",
    "\n",
    "# Load Data\n",
    "\n",
    "data_master=pd.read_csv('../data/house-prices-advanced-regression-techniques/train.csv')\n",
    "data = data_master.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c6e076",
   "metadata": {},
   "source": [
    "## 2 - Split Data into Train | Test | Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19a1177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split_test_train_validation(data: pd.DataFrame, test_size:int =0.2,\n",
    "                                     validation_size:int =0.2) -> pd.DataFrame:\n",
    "    # Split Train / Test\n",
    "    X = data.loc[:, data.columns != 'SalePrice']\n",
    "    y = data.SalePrice\n",
    "    \n",
    "    #First Split L between Train and Test \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        train_size= 1 - test_size,\n",
    "                                                        random_state = 42)\n",
    "    #Second Split :between Train and Validation \n",
    "    X_train, X_validation,y_train, y_validation = train_test_split(X_train, y_train,\n",
    "                                                                   train_size= 1 - validation_size,\n",
    "                                                                   random_state = 42)\n",
    "    # return all splitted data sets ( 6 sets )\n",
    "    return X_train, X_test,X_validation,y_train, y_test,y_validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2e2282",
   "metadata": {},
   "source": [
    "## 3 - Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aada6ec",
   "metadata": {},
   "source": [
    "\n",
    "## 3.1 - Preprocessing:  Check up for columns with missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ed190af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unwanted_columns(data: pd.DataFrame, to_remove_columns: list=[]) -> pd.DataFrame:\n",
    "    return data.drop(to_remove_columns, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c56fd2",
   "metadata": {},
   "source": [
    "## 3.2 - Preprocessing:  Encode Categorical Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a63da7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical_features(encoder,data: pd.DataFrame,is_test:bool =False) -> pd.DataFrame:\n",
    "    data_categorical = data.select_dtypes(include=['object']).columns\n",
    "    if not is_test :\n",
    "        filename=\"../models/encoder.joblib\" \n",
    "        encoder.fit(data[data_categorical])\n",
    "        dump(encoder ,filename)\n",
    "    data[data_categorical]=encoder.transform(data[data_categorical])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f10b99",
   "metadata": {},
   "source": [
    "## 3.3 - Preprocessing:   Fill Features with Null /NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4b984ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fill_features_nulls(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    data_numerical= data.select_dtypes([np.int64,np.float64]).columns\n",
    "    data_categorical = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "    data[data_numerical]=data[data_numerical].fillna(data[data_numerical].mean())\n",
    "    \n",
    "    for feature in data_categorical:\n",
    "        data[feature].interpolate(method ='linear', limit_direction ='forward', inplace=True)\n",
    "        data[feature].interpolate(method ='linear', limit_direction ='backward',inplace=True)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a86479",
   "metadata": {},
   "source": [
    "## 3.4 - Preprocessing:   Scale Data ( Standard Scaler )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fbafffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(scalar,data: pd.DataFrame,is_test:bool =False) -> pd.DataFrame:\n",
    "    if not is_test:\n",
    "        scalar.fit(data)\n",
    "        filename=\"../models/scalar.joblib\" \n",
    "        dump(scalar ,filename)\n",
    "    return pd.DataFrame(scalar.transform(data),columns = data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eab4666",
   "metadata": {},
   "source": [
    "## All data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8626630e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(data: pd.DataFrame,encoder,scalar,is_test:bool=False) -> pd.DataFrame:  \n",
    "   \n",
    "    # Carefully Selected Features ( after analysis)\n",
    "    list_of_features =[\"OverallQual\",\"GrLivArea\",\"GarageCars\",\"TotalBsmtSF\",\"1stFlrSF\",\n",
    "                       \"FullBath\",\"YearBuilt\",\"YearRemodAdd\",\"BsmtFinSF1\",\"Foundation\",\n",
    "                       \"LotFrontage\",\"WoodDeckSF\",\"MasVnrArea\",\"Fireplaces\",\n",
    "                       \"ExterQual\",\"BsmtQual\",\"KitchenQual\",\"GarageFinish\",\n",
    "                       \"GarageType\",\"HeatingQC\"]\n",
    "\n",
    "    # To remove Columns \n",
    "    unwanted_columns = [\"PoolQC\", \"MiscFeature\",\"Alley\",\"Fence\",\"FireplaceQu\"]\n",
    "    \n",
    "    data= drop_unwanted_columns(data,to_remove_columns=unwanted_columns)\n",
    "    \n",
    "    data =data[list_of_features]\n",
    "    \n",
    "    data= encode_categorical_features(encoder,data,is_test)\n",
    "    \n",
    "    data= fill_features_nulls(data)\n",
    "    \n",
    "    data= scale_data(scalar,data,is_test)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3363e8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Metrics and Exporting Data\n",
    "\n",
    "def compute_rmsle(y_true: np.ndarray, y_pred: np.ndarray, precision: int = 3) -> float:\n",
    "    rmsle = np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "    return round(rmsle, precision)\n",
    "\n",
    "def evaluate_performance(y_pred:np.ndarray,y_true: np.ndarray, precision: int = 2,\n",
    "                         comment:str = \"\") -> dict[str, str]: \n",
    "    y_pred=y_pred.ravel()\n",
    "    y_pred =abs(y_pred)\n",
    "    \n",
    "    y_true= y_true.ravel()\n",
    "   \n",
    "    rmse =compute_rmsle(y_true,y_pred,precision)\n",
    "    key =comment+\"_rmse\"\n",
    "    \n",
    "    return  dict({key : rmse})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11286e59",
   "metadata": {},
   "source": [
    "# Model building & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "924a1e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(data: pd.DataFrame) -> dict[str, str]:\n",
    "    \n",
    "    # split data into Train, Test, and Validation\n",
    "    X_train, X_test,X_validation,y_train, y_test,y_validation=data_split_test_train_validation(data)\n",
    "    \n",
    "    #Create Encoder\n",
    "    ordinal = OrdinalEncoder(handle_unknown=\"use_encoded_value\",unknown_value=np.nan)\n",
    "   \n",
    "    #Create Scalar\n",
    "    scalar =StandardScaler()\n",
    "    \n",
    "    #Preprocessing(cleaning data and training encoders,scalars)\n",
    "    X_train= data_preprocessing(X_train,encoder=ordinal,scalar=scalar,is_test=False)\n",
    "    \n",
    "    #Preprocessing(cleaning data and using trained encoders,scalars)\n",
    "    X_validation=data_preprocessing(X_validation,encoder=ordinal,scalar=scalar,is_test=True)\n",
    "    \n",
    "    #Define an evaluation dictonary\n",
    "    evaluations_dict= dict()\n",
    "\n",
    "    #Defining the Machine Learning model \n",
    "    LR_model = LinearRegression()\n",
    "    \n",
    "    #Train model\n",
    "    LR_model.fit(X_train, y_train)\n",
    "    model_filename=\"../models/model.joblib\" \n",
    "    dump(LR_model ,model_filename)\n",
    "    \n",
    "    #Validation-set evaluation\n",
    "    y_validation_predictions=LR_model.predict(X_validation)\n",
    "    validation_evaluation= evaluate_performance(y_pred=y_validation_predictions,y_true=y_validation,\n",
    "                                                precision=3,comment=\"Validation\")\n",
    "    evaluations_dict.update(validation_evaluation)\n",
    "    \n",
    "    # Model Build Evalution on Testing Set \n",
    "    #-------------------------------------\n",
    "    #Preprocessing(cleaning data and using trained encoders,scalars)\n",
    "    X_test=data_preprocessing(X_test,encoder=ordinal,scalar=scalar,is_test=True)\n",
    "    \n",
    "    #Testing-set evaluation\n",
    "    y_test_predictions=LR_model.predict(X_test)\n",
    "    test_evaluation= evaluate_performance(y_pred=y_test_predictions,y_true=y_test,\n",
    "                                                precision=3,comment=\"Test\")\n",
    "    evaluations_dict.update(test_evaluation)\n",
    "    # Returns a dictionary with the model performances (for example {'rmse': 0.18})\n",
    "    return evaluations_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eca2f7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Validation_rmse': 0.163, 'Test_rmse': 0.186}\n"
     ]
    }
   ],
   "source": [
    "# Build Model \n",
    "evaluations= build_model(data)\n",
    "print(evaluations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8baccff",
   "metadata": {},
   "source": [
    "# Model inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d8c4fb",
   "metadata": {},
   "source": [
    "## Reading File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7d7b7d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "test_master=pd.read_csv('../data/house-prices-advanced-regression-techniques/test.csv')\n",
    "test_data = test_master.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a3dcf710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(input_data: pd.DataFrame) -> np.ndarray:\n",
    "    \n",
    "    # load the encoder \n",
    "    encoder_filename=\"../models/encoder.joblib\" \n",
    "    ordinal =load(encoder_filename)\n",
    "\n",
    "    # load the scalar\n",
    "    scalar_filename=\"../models/scalar.joblib\" \n",
    "    scalar=load(scalar_filename)\n",
    "\n",
    "    # load the model\n",
    "    model_filename=\"../models/model.joblib\" \n",
    "    model= load(model_filename)\n",
    "    \n",
    "    input_data=data_preprocessing(input_data,encoder=ordinal,scalar=scalar,is_test=True)\n",
    "    \n",
    "    #Validation-set evaluation\n",
    "    y_predictions=model.predict(input_data)\n",
    "    \n",
    "    return y_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f009a08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101484.43003764 157864.03395506 180538.79709244 ... 160823.63999707\n",
      " 103964.05876926 236273.53828862]\n"
     ]
    }
   ],
   "source": [
    "predicitons =make_predictions(test_master)\n",
    "print(predicitons)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
